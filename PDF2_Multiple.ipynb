{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb81ce-c34e-4881-bcc5-024896f56c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2240b8a7-52d0-4493-9a20-5a9e94afc3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get file paths from the user\n",
    "def get_file_paths():\n",
    "    print(\"Enter the paths of the files you want to process (PNG or PDF).\")\n",
    "    print(\"Press Enter after each file. When you're done, just press Enter without typing a file path.\")\n",
    "    \n",
    "    file_paths = []\n",
    "    while True:\n",
    "        file_path = input(\"Enter file path (or press Enter to finish): \").strip()\n",
    "        if not file_path:  # Stop when the user presses Enter without input\n",
    "            break\n",
    "        if file_path.lower().endswith(('.png', '.pdf')):  # Accept only PNG or PDF files\n",
    "            file_paths.append(file_path)\n",
    "        else:\n",
    "            print(\"Invalid file type. Please provide a PNG or PDF file.\")\n",
    "    \n",
    "    return file_paths\n",
    "\n",
    "# Call the function and store the selected file paths\n",
    "global files_to_process  # Make the variable global so it can be accessed in other cells\n",
    "files_to_process = get_file_paths()\n",
    "\n",
    "# Display the selected files\n",
    "if files_to_process:\n",
    "    print(\"Selected files:\")\n",
    "    for file in files_to_process:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"No files selected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337baefa-5ea0-42bb-831b-ca7394eb36be",
   "metadata": {},
   "source": [
    "### Instructions for Running the Notebook\n",
    "\n",
    "#### Overview\n",
    "This notebook processes PNG and PDF files to extract specific features. If a PDF is provided, it will first be converted into PNG images, with each page saved as a separate PNG. The resulting PNGs will then be processed alongside any other provided PNG files.\n",
    "\n",
    "#### Workflow\n",
    "1. **Input Files**:\n",
    "   - Users can upload PNG and PDF files.\n",
    "   - If a PNG is provided, it will be processed directly without modification.\n",
    "   - If a PDF is provided, it will be converted into PNG images, one for each page.\n",
    "\n",
    "2. **PDF to PNG Conversion**:\n",
    "   - **Library Used**: The `pdf2image` library is used for converting PDF pages into PNG images.\n",
    "   - **Naming Convention**: Each page of the PDF is converted to a PNG file and saved in the same directory as the PDF, within a folder named `converted_files`. The naming format is:\n",
    "     ```\n",
    "     <original_filename>_convertedPNG_pg<page_number>.png\n",
    "     ```\n",
    "     For example, a PDF named `document.pdf` with 3 pages will generate:\n",
    "     ```\n",
    "     converted_files/document_convertedPNG_pg1.png\n",
    "     converted_files/document_convertedPNG_pg2.png\n",
    "     converted_files/document_convertedPNG_pg3.png\n",
    "     ```\n",
    "   - **Integration**: The paths of the converted PNG files are automatically added to the `files_to_process` list. This ensures seamless processing in subsequent steps.\n",
    "\n",
    "3. **Processing**:\n",
    "   - All PNG files, whether uploaded directly or generated from PDFs, are processed together in the final step.\n",
    "\n",
    "#### Prerequisites\n",
    "1. **Install Required Libraries**:\n",
    "   Ensure the following Python libraries are installed:\n",
    "   ```bash\n",
    "   pip install pdf2image opencv-python-headless numpy\n",
    "   ```\n",
    "2. **Install Poppler for PDF Processing**:\n",
    "   - **macOS (using Homebrew):**\n",
    "     ```bash\n",
    "     brew install poppler\n",
    "     ```\n",
    "   - **Linux (using APT):**\n",
    "     ```bash\n",
    "     sudo apt-get install poppler-utils\n",
    "     ```\n",
    "   - **Windows**:\n",
    "     Download Poppler from [Poppler for Windows](https://blog.alivate.com.au/poppler-windows/), extract it, and add the `bin` folder to your system PATH.\n",
    "\n",
    "#### Steps to Execute\n",
    "1. **Run Cell 1**:\n",
    "   - Input the file paths for the PNG and/or PDF files you want to process. Press Enter after each path. When finished, press Enter on an empty line.\n",
    "2. **Run Cell 2**:\n",
    "   - Converts all PDF files into PNGs. Converted PNGs are saved in a `converted_files` folder next to the respective PDFs.\n",
    "   - All PNG paths (both directly uploaded and converted from PDFs) are added to `files_to_process`.\n",
    "3. **Run Cell 3**:\n",
    "   - Processes all PNG files in `files_to_process`.\n",
    "\n",
    "#### Notes\n",
    "- Ensure the file paths provided in **Cell 1** are correct.\n",
    "- Any errors during processing will be logged in the console.\n",
    "- The processed output (e.g., annotated PNGs) will be saved in designated folders based on your workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4785a15-f5e0-4419-8a86-4eafc3875559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "\n",
    "def convert_pdf_to_png(file_path, dpi=900):  # Default DPI set to 900\n",
    "    \"\"\"\n",
    "    Converts a PDF file into PNG images, one per page, at 900 DPI resolution.\n",
    "    Saves the images in a 'converted_files' directory next to the PDF.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = os.path.join(os.path.dirname(file_path), \"converted_files\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Extract the base name of the file (without extension)\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # Convert PDF to a list of images at the specified DPI\n",
    "    try:\n",
    "        images = convert_from_path(file_path, dpi=dpi)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Could not convert PDF: {file_path}. Error: {e}\")\n",
    "        return []\n",
    "\n",
    "    output_paths = []\n",
    "\n",
    "    # Save each page as a PNG\n",
    "    for page_num, image in enumerate(images, start=1):\n",
    "        output_path = os.path.join(output_dir, f\"{base_name}_convertedPNG_pg{page_num}.png\")\n",
    "        image.save(output_path, \"PNG\")\n",
    "        output_paths.append(output_path)\n",
    "        print(f\"[INFO] Saved: {output_path}\")\n",
    "\n",
    "    return output_paths\n",
    "\n",
    "def process_pdfs(file_paths):\n",
    "    \"\"\"\n",
    "    Process multiple PDF files, convert their pages to PNG,\n",
    "    and update the global list 'files_to_process' with the converted PNG paths.\n",
    "    \"\"\"\n",
    "    if not file_paths:\n",
    "        print(\"[INFO] No PDF files to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] PDF files to process: {len(file_paths)}\")\n",
    "    all_converted_paths = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"[ERROR] File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Processing PDF: {file_path}\")\n",
    "        converted_paths = convert_pdf_to_png(file_path)\n",
    "        all_converted_paths.extend(converted_paths)\n",
    "\n",
    "    # Update the global 'files_to_process' to include the converted PNGs\n",
    "    files_to_process.extend(all_converted_paths)\n",
    "\n",
    "    print(\"\\n[INFO] PDF conversion completed.\")\n",
    "    return all_converted_paths\n",
    "\n",
    "# Filter out PDF files from the user-provided file list and process them\n",
    "if 'files_to_process' in globals():\n",
    "    pdf_files = [file for file in files_to_process if file.lower().endswith('.pdf')]\n",
    "    process_pdfs(pdf_files)\n",
    "else:\n",
    "    print(\"[ERROR] 'files_to_process' not found. Please run Cell 1 first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc191dc-1839-4d1d-b361-9ae83dc6afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def process_image(image_path):\n",
    "    \"\"\"Process a single image to create the temp.png file.\"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load image: {image_path}\")\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply binary thresholding\n",
    "    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Detect contours (these should correspond to boxes)\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Create a blank image to draw the contours (for visualization purposes)\n",
    "    contour_image = np.zeros_like(image)\n",
    "    cv2.drawContours(contour_image, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "    flowchart_image = np.zeros_like(contour_image)\n",
    "\n",
    "    # Iterate through each contour and apply filtering\n",
    "    for contour in contours:\n",
    "        # Calculate the contour's perimeter (arcLength) to identify small/dotted lines\n",
    "        contour_length = cv2.arcLength(contour, True)\n",
    "        \n",
    "        # Get the bounding rectangle for each contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Set thresholds to filter out text and dotted lines\n",
    "        min_contour_length = 200  # Adjust this value based on your image (for dotted lines)\n",
    "        min_text_width = 20  # Minimum width for potential text (adjust based on image)\n",
    "        min_text_height = 20  # Minimum height for potential text (adjust based on image)\n",
    "        \n",
    "        # Filter out small/dotted lines and text contours\n",
    "        if contour_length > min_contour_length and (w > min_text_width and h > min_text_height):\n",
    "            # Draw remaining contours (that are not filtered out)\n",
    "            cv2.drawContours(flowchart_image, [contour], -1, (0, 0, 255), 5)\n",
    "\n",
    "    # Get the directory of the selected image\n",
    "    output_dir = os.path.join(os.path.dirname(image_path), \"processed_files\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create the output path\n",
    "    output_path = os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(image_path))[0]}_temp.png\")\n",
    "\n",
    "    # Save the temp image to the same location\n",
    "    cv2.imwrite(output_path, flowchart_image)\n",
    "\n",
    "    print(f\"Temporary file saved at: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "def process_files(file_paths):\n",
    "    \"\"\"Process multiple files to create temp images for all.\"\"\"\n",
    "    if not file_paths:\n",
    "        print(\"[INFO] No files to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Files to process: {len(file_paths)}\")\n",
    "    temp_image_paths = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Ensure only PNG files are processed\n",
    "        if not file_path.lower().endswith('.png'):\n",
    "            print(f\"[WARNING] Skipping non-PNG file: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"[ERROR] File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Processing file: {file_path}\")\n",
    "        try:\n",
    "            temp_path = process_image(file_path)\n",
    "            temp_image_paths.append(temp_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process file {file_path}: {e}\")\n",
    "\n",
    "    print(\"\\n[INFO] All files processed successfully.\")\n",
    "    return temp_image_paths\n",
    "\n",
    "# Ensure this part is executed to process multiple files\n",
    "if 'files_to_process' in globals():\n",
    "    temp_files = process_files(files_to_process)\n",
    "else:\n",
    "    print(\"[ERROR] 'files_to_process' not found. Please run Cell 1 first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f7ecf-b1f8-4e0b-8f71-f541c6fca029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Initialize PaddleOCR\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "\n",
    "# Function to check if a contour is a rectangle\n",
    "def is_rectangle(approx):\n",
    "    if len(approx) == 4:\n",
    "        if cv2.isContourConvex(approx):\n",
    "            x, y, w, h = cv2.boundingRect(approx)\n",
    "            angles = []\n",
    "            for i in range(4):\n",
    "                p1, p2, p3 = approx[i][0], approx[(i+1) % 4][0], approx[(i+2) % 4][0]\n",
    "                angle = np.arctan2(p2[1] - p1[1], p2[0] - p1[0]) - np.arctan2(p3[1] - p2[1], p3[0] - p2[0])\n",
    "                angle = np.abs(np.degrees(angle))\n",
    "                if angle > 180:\n",
    "                    angle = 360 - angle\n",
    "                angles.append(angle)\n",
    "            return all(np.isclose(angle, 90, atol=10) for angle in angles)\n",
    "    return False\n",
    "\n",
    "def process_image(image_path):\n",
    "    \"\"\"Process a single temp.png image.\"\"\"\n",
    "    # Read the image\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Adjust Canny edge detection thresholds\n",
    "    edges = cv2.Canny(gray, 50, 200, apertureSize=3)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Process contours\n",
    "    rectangles = []\n",
    "    for contour in contours:\n",
    "        epsilon = 0.02 * cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "        if is_rectangle(approx):\n",
    "            rectangles.append(approx)\n",
    "\n",
    "    # Prepare to store text information\n",
    "    rectangle_texts = []\n",
    "\n",
    "    # Extract text from detected rectangles\n",
    "    for i, rect in enumerate(rectangles):\n",
    "        x, y, w, h = cv2.boundingRect(rect)\n",
    "\n",
    "        # Crop the rectangle region\n",
    "        cropped_img = img[y:y + h, x:x + w]\n",
    "\n",
    "        try:\n",
    "            # Perform OCR only within the detected rectangle\n",
    "            ocr_results = ocr.ocr(cropped_img, cls=True)\n",
    "            extracted_text = \" \".join([line[1][0] for line in ocr_results[0]])\n",
    "\n",
    "            # Store the text along with rectangle label\n",
    "            rectangle_texts.append({\"Rectangle No\": i + 1, \"Text\": extracted_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            rectangle_texts.append({\"Rectangle No\": i + 1, \"Text\": f\"Error during OCR: {e}\"})\n",
    "\n",
    "    # Save extracted text to a file\n",
    "    output_dir = \"processed_files\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    text_output_path = os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(image_path))[0]}_text.txt\")\n",
    "    with open(text_output_path, 'w') as f:\n",
    "        for entry in rectangle_texts:\n",
    "            f.write(f\"Rectangle {entry['Rectangle No']}: {entry['Text']}\\n\")\n",
    "    print(f\"[INFO] Extracted text saved at: {text_output_path}\")\n",
    "\n",
    "    # Display extracted text for each rectangle\n",
    "    for entry in rectangle_texts:\n",
    "        print(f\"Rectangle {entry['Rectangle No']}: {entry['Text']}\")\n",
    "\n",
    "    # Display the labeled rectangles for verification\n",
    "    debug_img = img.copy()\n",
    "    for i, rect in enumerate(rectangles):\n",
    "        x, y, w, h = cv2.boundingRect(rect)\n",
    "        cv2.rectangle(debug_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(debug_img, str(i + 1), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 0), 3)\n",
    "\n",
    "    # Save the labeled image\n",
    "    labeled_image_path = os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(image_path))[0]}_labeled.png\")\n",
    "    cv2.imwrite(labeled_image_path, debug_img)\n",
    "    print(f\"[INFO] Labeled image saved at: {labeled_image_path}\")\n",
    "\n",
    "    # Display the image with labeled rectangles\n",
    "    plt.figure(figsize=(100, 100))\n",
    "    plt.title(\"Detected Rectangles with Labels\")\n",
    "    plt.imshow(cv2.cvtColor(debug_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def process_files(temp_image_paths):\n",
    "    \"\"\"Process multiple temp images.\"\"\"\n",
    "    if not temp_image_paths:\n",
    "        print(\"[INFO] No files to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Processing {len(temp_image_paths)} files...\")\n",
    "    for image_path in temp_image_paths:\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"[ERROR] File does not exist: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Processing file: {image_path}\")\n",
    "        process_image(image_path)\n",
    "\n",
    "    print(\"[INFO] Processing completed for all files.\")\n",
    "\n",
    "# Ensure this part is executed to process multiple files\n",
    "if 'temp_files' in globals():\n",
    "    process_files(temp_files)\n",
    "else:\n",
    "    print(\"[ERROR] 'temp_files' not found. Please run the previous cell to generate temp images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f907591-c757-4952-a0b8-ae7214e1b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Initialize PaddleOCR\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "\n",
    "# Global list to store paths of generated JSON files with rectangles\n",
    "if 'rectangle_json_files' not in globals():\n",
    "    rectangle_json_files = []\n",
    "\n",
    "# Function to check if a contour is a rectangle\n",
    "def is_rectangle(approx):\n",
    "    if len(approx) == 4:\n",
    "        if cv2.isContourConvex(approx):\n",
    "            x, y, w, h = cv2.boundingRect(approx)\n",
    "            angles = []\n",
    "            for i in range(4):\n",
    "                p1, p2, p3 = approx[i][0], approx[(i+1) % 4][0], approx[(i+2) % 4][0]\n",
    "                angle = np.arctan2(p2[1] - p1[1], p2[0] - p1[0]) - np.arctan2(p3[1] - p2[1], p3[0] - p2[0])\n",
    "                angle = np.abs(np.degrees(angle))\n",
    "                if angle > 180:\n",
    "                    angle = 360 - angle\n",
    "                angles.append(angle)\n",
    "            return all(np.isclose(angle, 90, atol=10) for angle in angles)\n",
    "    return False\n",
    "\n",
    "def process_file(original_image_path, temp_image_path):\n",
    "    \"\"\"Process a single file: extract text from temp and original images.\"\"\"\n",
    "    print(f\"[INFO] Processing file: {original_image_path} with {temp_image_path}\")\n",
    "\n",
    "    # Load the images\n",
    "    processed_img = cv2.imread(temp_image_path)\n",
    "    original_img = cv2.imread(original_image_path)\n",
    "\n",
    "    # Convert processed image to grayscale for contour detection\n",
    "    gray = cv2.cvtColor(processed_img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 200, apertureSize=3)\n",
    "\n",
    "    # Find contours in the processed image\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Detect rectangles\n",
    "    rectangles = []\n",
    "    for contour in contours:\n",
    "        epsilon = 0.02 * cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "        if is_rectangle(approx):\n",
    "            rectangles.append(approx)\n",
    "\n",
    "    # Prepare to store text information along with coordinates\n",
    "    rectangle_texts = []\n",
    "\n",
    "    # Extract text from original image based on detected rectangles\n",
    "    for i, rect in enumerate(rectangles):\n",
    "        x, y, w, h = cv2.boundingRect(rect)\n",
    "\n",
    "        # Crop the corresponding region from the original image\n",
    "        cropped_img = original_img[y:y + h, x:x + w]\n",
    "\n",
    "        try:\n",
    "            # Perform OCR on the cropped region\n",
    "            ocr_results = ocr.ocr(cropped_img, cls=True)\n",
    "            extracted_text = \" \".join([line[1][0] for line in ocr_results[0]])\n",
    "\n",
    "            # Store text with rectangle label and coordinates\n",
    "            rectangle_texts.append({\n",
    "                \"Rectangle No\": i + 1,\n",
    "                \"Text\": extracted_text,\n",
    "                \"Coordinates\": [(x, y), (x + w, y), (x + w, y + h), (x, y + h)]  # Storing the 4 corners\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            rectangle_texts.append({\n",
    "                \"Rectangle No\": i + 1,\n",
    "                \"Text\": f\"Error during OCR: {e}\",\n",
    "                \"Coordinates\": []  # In case OCR fails, no coordinates are stored\n",
    "            })\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    output_dir = \"processed_files\"  # Create the processed_files folder directly in the current working directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    json_output_path = os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(original_image_path))[0]}_text.json\")\n",
    "    output_data = {\"rectangles\": rectangle_texts}\n",
    "    with open(json_output_path, \"w\") as outfile:\n",
    "        json.dump(output_data, outfile, indent=4)\n",
    "    print(f\"[INFO] Extracted text and coordinates saved at: {json_output_path}\")\n",
    "\n",
    "    # Add the JSON file path to the global rectangle_json_files list\n",
    "    rectangle_json_files.append(json_output_path)\n",
    "\n",
    "    # Display the labelled rectangles on the processed image for verification\n",
    "    debug_img = processed_img.copy()\n",
    "    for i, rect in enumerate(rectangles):\n",
    "        x, y, w, h = cv2.boundingRect(rect)\n",
    "        cv2.rectangle(debug_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(debug_img, str(i + 1), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 0), 3)\n",
    "\n",
    "    # Save the labelled image\n",
    "    labeled_image_path = os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(original_image_path))[0]}_labeled.png\")\n",
    "    cv2.imwrite(labeled_image_path, debug_img)\n",
    "    print(f\"[INFO] Labeled image saved at: {labeled_image_path}\")\n",
    "\n",
    "    # Display the image with labeled rectangles\n",
    "    plt.figure(figsize=(100, 100))\n",
    "    plt.title(\"Labelled Rectangles on Processed Image\")\n",
    "    plt.imshow(cv2.cvtColor(debug_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Print the extracted text and coordinates for each rectangle\n",
    "    print(\"Extracted Text and Coordinates from Original Image:\")\n",
    "    for entry in rectangle_texts:\n",
    "        print(f\"Rectangle {entry['Rectangle No']}: {entry['Text']}\")\n",
    "        print(f\"Coordinates: {entry['Coordinates']}\")\n",
    "\n",
    "def process_files(original_image_paths, temp_image_paths):\n",
    "    \"\"\"Process multiple files using their respective original and temp images.\"\"\"\n",
    "    if not original_image_paths or not temp_image_paths:\n",
    "        print(\"[INFO] No files to process.\")\n",
    "        return\n",
    "\n",
    "    for original_path, temp_path in zip(original_image_paths, temp_image_paths):\n",
    "        if not os.path.exists(original_path) or not os.path.exists(temp_path):\n",
    "            print(f\"[ERROR] File not found: {original_path} or {temp_path}\")\n",
    "            continue\n",
    "\n",
    "        process_file(original_path, temp_path)\n",
    "\n",
    "    print(\"[INFO] Processing completed for all files.\")\n",
    "    print(f\"[INFO] All generated JSON files with rectangles: {rectangle_json_files}\")\n",
    "\n",
    "# Ensure this part is executed to process multiple files\n",
    "if 'files_to_process' in globals() and 'temp_files' in globals():\n",
    "    process_files(files_to_process, temp_files)\n",
    "else:\n",
    "    print(\"[ERROR] 'files_to_process' or 'temp_files' not found. Please run previous cells to generate the required data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ff3c88-7772-4706-8aad-2e23b60a0635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Global variables\n",
    "processed_file_results = []\n",
    "detected_line_files = []\n",
    "debug_images = []\n",
    "\n",
    "# Function to check if a contour is a rectangle\n",
    "def is_rectangle(approx):\n",
    "    \"\"\"Check if a contour approximates a rectangle.\"\"\"\n",
    "    if len(approx) == 4:\n",
    "        if cv2.isContourConvex(approx):\n",
    "            x, y, w, h = cv2.boundingRect(approx)\n",
    "            angles = []\n",
    "            for i in range(4):\n",
    "                p1, p2, p3 = approx[i][0], approx[(i+1) % 4][0], approx[(i+2) % 4][0]\n",
    "                angle = np.arctan2(p2[1] - p1[1], p2[0] - p1[0]) - np.arctan2(p3[1] - p2[1], p3[0] - p2[0])\n",
    "                angle = np.abs(np.degrees(angle))\n",
    "                if angle > 180:\n",
    "                    angle = 360 - angle\n",
    "                angles.append(angle)\n",
    "            return all(np.isclose(angle, 90, atol=10) for angle in angles)\n",
    "    return False\n",
    "\n",
    "def process_file(image_path):\n",
    "    \"\"\"Process a single image file to detect lines and rectangles.\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"[ERROR] Image not found: {image_path}\")\n",
    "        return None\n",
    "\n",
    "    # Extract the base name of the file\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply edge detection\n",
    "    edges = cv2.Canny(gray, 50, 200, apertureSize=3)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Create blank images for masks and debug visualization\n",
    "    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "    debug_img = img.copy()\n",
    "\n",
    "    # Detect rectangles\n",
    "    rectangles = []\n",
    "    for contour in contours:\n",
    "        epsilon = 0.02 * cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "        if is_rectangle(approx):\n",
    "            rectangles.append(approx)\n",
    "\n",
    "    # Draw rectangles on debug image and mask\n",
    "    for i, rect in enumerate(rectangles):\n",
    "        cv2.drawContours(mask, [rect], 0, (255), -1)\n",
    "        cv2.drawContours(debug_img, [rect], 0, (0, 255, 0), 4)\n",
    "\n",
    "        # Label each rectangle\n",
    "        M = cv2.moments(rect)\n",
    "        if M[\"m00\"] != 0:\n",
    "            cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "        else:\n",
    "            cX, cY = 0, 0\n",
    "        cv2.putText(debug_img, str(i+1), (cX, cY), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 0), 3)\n",
    "\n",
    "    # Create an inverted mask\n",
    "    inverted_mask = cv2.bitwise_not(mask)\n",
    "\n",
    "    # Remove rectangles using the inverted mask\n",
    "    result_no_rectangles = cv2.bitwise_and(img, img, mask=inverted_mask)\n",
    "\n",
    "    # Fine-tune the mask\n",
    "    kernel = np.ones((10, 10), np.uint8)\n",
    "    dilated_mask = cv2.dilate(mask, kernel, iterations=2)\n",
    "    inverted_dilated_mask = cv2.bitwise_not(dilated_mask)\n",
    "    result_clean = cv2.bitwise_and(img, img, mask=inverted_dilated_mask)\n",
    "\n",
    "    # Detect edges in the cleaned image\n",
    "    gray_clean = cv2.cvtColor(result_clean, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray_clean, (5, 5), 0)\n",
    "    edges_clean = cv2.Canny(blurred, 50, 150, apertureSize=3)\n",
    "\n",
    "    # Detect lines using Hough Line Transform\n",
    "    lines = cv2.HoughLinesP(edges_clean, rho=1, theta=np.pi / 180, threshold=30, minLineLength=30, maxLineGap=20)\n",
    "\n",
    "    # Draw detected lines\n",
    "    line_img = result_clean.copy()\n",
    "    if lines is not None:\n",
    "        print(f\"[INFO] Number of lines detected: {len(lines)}\")\n",
    "        for idx, line in enumerate(lines):\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            cv2.line(line_img, (x1, y1), (x2, y2), (0, 0, 255), 3)\n",
    "    else:\n",
    "        print(\"[INFO] No lines detected.\")\n",
    "\n",
    "    # Save results\n",
    "    output_dir = \"processed_files\"  # Create the processed_files folder in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    debug_path = os.path.join(output_dir, f\"{base_name}_debug_rectangles.png\")\n",
    "    no_rectangles_path = os.path.join(output_dir, f\"{base_name}_no_rectangles.png\")\n",
    "    cleaned_path = os.path.join(output_dir, f\"{base_name}_cleaned.png\")\n",
    "    lines_detected_path = os.path.join(output_dir, f\"{base_name}_lines_detected.png\")\n",
    "\n",
    "    cv2.imwrite(debug_path, debug_img)\n",
    "    cv2.imwrite(no_rectangles_path, result_no_rectangles)\n",
    "    cv2.imwrite(cleaned_path, result_clean)\n",
    "    cv2.imwrite(lines_detected_path, line_img)\n",
    "\n",
    "    # Store paths in global variables\n",
    "    processed_file_results.append({\n",
    "        \"file\": base_name,\n",
    "        \"debug_rectangles\": debug_path,\n",
    "        \"no_rectangles\": no_rectangles_path,\n",
    "        \"cleaned\": cleaned_path,\n",
    "        \"lines_detected\": lines_detected_path,\n",
    "    })\n",
    "    debug_images.append(debug_img)\n",
    "\n",
    "    # Display results\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 20))\n",
    "    fig.suptitle(f\"Results for {base_name}\", fontsize=16)\n",
    "\n",
    "    axs[0, 0].imshow(cv2.cvtColor(debug_img, cv2.COLOR_BGR2RGB))\n",
    "    axs[0, 0].set_title(\"Debug Rectangles\")\n",
    "    axs[0, 0].axis('off')\n",
    "\n",
    "    axs[0, 1].imshow(cv2.cvtColor(result_no_rectangles, cv2.COLOR_BGR2RGB))\n",
    "    axs[0, 1].set_title(\"Image Without Rectangles\")\n",
    "    axs[0, 1].axis('off')\n",
    "\n",
    "    axs[1, 0].imshow(cv2.cvtColor(result_clean, cv2.COLOR_BGR2RGB))\n",
    "    axs[1, 0].set_title(\"Cleaned Image\")\n",
    "    axs[1, 0].axis('off')\n",
    "\n",
    "    axs[1, 1].imshow(cv2.cvtColor(line_img, cv2.COLOR_BGR2RGB))\n",
    "    axs[1, 1].set_title(\"Detected Lines\")\n",
    "    axs[1, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"[INFO] Debug image saved at {debug_path}\")\n",
    "\n",
    "def process_files(temp_image_paths):\n",
    "    \"\"\"Process multiple image files.\"\"\"\n",
    "    global detected_line_files\n",
    "    if not temp_image_paths:\n",
    "        print(\"[INFO] No files to process.\")\n",
    "        return\n",
    "\n",
    "    for image_path in temp_image_paths:\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"[ERROR] File not found: {image_path}\")\n",
    "            continue\n",
    "        process_file(image_path)\n",
    "\n",
    "    detected_line_files = [result[\"lines_detected\"] for result in processed_file_results]\n",
    "    print(f\"[INFO] Processed {len(detected_line_files)} files.\")\n",
    "\n",
    "# Ensure the function is executed\n",
    "if 'temp_files' in globals():\n",
    "    process_files(temp_files)\n",
    "else:\n",
    "    print(\"[ERROR] 'temp_files' not found. Please provide a valid list of image paths.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870895e-9069-4625-94e9-28e90718e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Helper function to calculate the distance between two points\n",
    "def distance(p1, p2):\n",
    "    return np.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n",
    "\n",
    "# Helper function to calculate the midpoint of a line\n",
    "def midpoint(p1, p2):\n",
    "    return ((p1[0] + p2[0]) // 2, (p1[1] + p2[1]) // 2)\n",
    "\n",
    "# Helper function to check if two points are close\n",
    "def points_are_close(p1, p2, distance_thresh=10):\n",
    "    return distance(p1, p2) < distance_thresh\n",
    "\n",
    "# Update function to combine lines based on both proximity and overlapping points\n",
    "def should_combine(line1, line2, distance_thresh=10):\n",
    "    if (points_are_close(line1[0], line2[0], distance_thresh) or \n",
    "        points_are_close(line1[1], line2[1], distance_thresh) or \n",
    "        points_are_close(line1[0], line2[1], distance_thresh) or \n",
    "        points_are_close(line1[1], line2[0], distance_thresh)):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to combine two lines\n",
    "def combine_lines(line1, line2):\n",
    "    points = [line1[0], line1[1], line2[0], line2[1]]\n",
    "    max_dist = 0\n",
    "    start_point = points[0]\n",
    "    end_point = points[1]\n",
    "    for i in range(len(points)):\n",
    "        for j in range(i + 1, len(points)):\n",
    "            dist = distance(points[i], points[j])\n",
    "            if dist > max_dist:\n",
    "                max_dist = dist\n",
    "                start_point = points[i]\n",
    "                end_point = points[j]\n",
    "    return (start_point, end_point)\n",
    "\n",
    "# Global list to store JSON file paths\n",
    "if 'json_files' not in globals():\n",
    "    json_files = []\n",
    "\n",
    "def process_file(original_image_path):\n",
    "    \"\"\"Process a single file to detect and combine lines.\"\"\"\n",
    "    img = cv2.imread(original_image_path)\n",
    "    if img is None:\n",
    "        print(f\"[ERROR] Image not found: {original_image_path}\")\n",
    "        return None\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(original_image_path))[0]\n",
    "    gray_clean = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edges_clean = cv2.Canny(gray_clean, 10, 150, apertureSize=7, L2gradient=True)\n",
    "\n",
    "    lines = cv2.HoughLinesP(edges_clean, rho=1, theta=np.pi / 180, threshold=10, minLineLength=5, maxLineGap=15)\n",
    "\n",
    "    line_img = img.copy()\n",
    "    line_points = []\n",
    "    if lines is not None:\n",
    "        for idx, line in enumerate(lines):\n",
    "            x1, y1, x2, y2 = map(int, line[0])\n",
    "            cv2.line(line_img, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "            line_points.append(((x1, y1), (x2, y2)))\n",
    "            mid_x, mid_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "            cv2.putText(line_img, f'{idx+1}', (mid_x, mid_y), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "    combined_lines = []\n",
    "    for line1 in line_points:\n",
    "        merged = False\n",
    "        for i, line2 in enumerate(combined_lines):\n",
    "            if should_combine(line1, line2):\n",
    "                combined_lines[i] = combine_lines(line1, line2)\n",
    "                merged = True\n",
    "                break\n",
    "        if not merged:\n",
    "            combined_lines.append(line1)\n",
    "\n",
    "    line_img_combined = img.copy()\n",
    "    for idx, line in enumerate(combined_lines):\n",
    "        x1, y1 = line[0]\n",
    "        x2, y2 = line[1]\n",
    "        cv2.line(line_img_combined, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        mid_x, mid_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "        cv2.putText(line_img_combined, f'{idx+1}', (mid_x, mid_y), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "    json_output_path = os.path.join(os.path.dirname(original_image_path), f\"{base_name}_lines.json\")\n",
    "    line_data = {\n",
    "        \"lines\": [{\"start\": tuple(map(int, line[0])), \"end\": tuple(map(int, line[1]))} for line in line_points],\n",
    "        \"combined_lines\": [{\"start\": tuple(map(int, line[0])), \"end\": tuple(map(int, line[1]))} for line in combined_lines]\n",
    "    }\n",
    "    with open(json_output_path, \"w\") as f:\n",
    "        json.dump(line_data, f, indent=4)\n",
    "    print(f\"[INFO] Line data saved at: {json_output_path}\")\n",
    "\n",
    "    # Append JSON file path to the global list\n",
    "    json_files.append(json_output_path)\n",
    "\n",
    "    cv2.imwrite(os.path.join(os.path.dirname(original_image_path), f\"{base_name}_lines.png\"), line_img)\n",
    "    cv2.imwrite(os.path.join(os.path.dirname(original_image_path), f\"{base_name}_combined_lines.png\"), line_img_combined)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(96, 64))\n",
    "    fig.suptitle(f\"Line Detection for {base_name}\", fontsize=36)\n",
    "\n",
    "    axs[0].imshow(cv2.cvtColor(line_img, cv2.COLOR_BGR2RGB))\n",
    "    axs[0].set_title(\"Detected Lines\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(cv2.cvtColor(line_img_combined, cv2.COLOR_BGR2RGB))\n",
    "    axs[1].set_title(\"Combined Lines\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Detected {len(combined_lines)} combined lines.\")\n",
    "    for idx, points in enumerate(combined_lines):\n",
    "        print(f\"Combined Line {idx+1}: Start {points[0]}, End {points[1]}\")\n",
    "\n",
    "def process_files(image_paths):\n",
    "    \"\"\"Process multiple files for line detection.\"\"\"\n",
    "    if not image_paths:\n",
    "        print(\"[INFO] No files to process.\")\n",
    "        return\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"[ERROR] File not found: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        process_file(image_path)\n",
    "\n",
    "    print(\"[INFO] Line detection completed for all files.\")\n",
    "    print(f\"[INFO] JSON files generated: {json_files}\")\n",
    "\n",
    "if 'detected_line_files' in globals():\n",
    "    process_files(detected_line_files)\n",
    "else:\n",
    "    print(\"[ERROR] 'detected_line_files' not found. Please run the previous cell to generate detected lines images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e379ce8-5378-4819-bf27-b0486b8d7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging code to check files in temp_files and json_files\n",
    "if 'temp_files' in globals():\n",
    "    print(\"[INFO] Checking files in temp_files...\")\n",
    "    for file in temp_files:\n",
    "        if os.path.exists(file):\n",
    "            print(f\"[FOUND] {file}\")\n",
    "        else:\n",
    "            print(f\"[MISSING] {file}\")\n",
    "else:\n",
    "    print(\"[ERROR] 'temp_files' is not defined.\")\n",
    "\n",
    "if 'json_files' in globals():\n",
    "    print(\"[INFO] Checking files in json_files...\")\n",
    "    for file in json_files:\n",
    "        if os.path.exists(file):\n",
    "            print(f\"[FOUND] {file}\")\n",
    "        else:\n",
    "            print(f\"[MISSING] {file}\")\n",
    "else:\n",
    "    print(\"[ERROR] 'json_files' is not defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f5e58-806f-4efb-ac50-b141c8b7aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Helper function to calculate the distance between two points\n",
    "def distance(p1, p2):\n",
    "    return np.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n",
    "\n",
    "# Helper function to calculate the midpoint of a line\n",
    "def midpoint(p1, p2):\n",
    "    return ((p1[0] + p2[0]) // 2, (p1[1] + p2[1]) // 2)\n",
    "\n",
    "# Helper function to check if two points are close\n",
    "def points_are_close(p1, p2, distance_thresh=10):\n",
    "    return distance(p1, p2) < distance_thresh\n",
    "\n",
    "# Update function to combine lines based on both proximity and overlapping points\n",
    "def should_combine(line1, line2, distance_thresh=10):\n",
    "    if (points_are_close(line1[0], line2[0], distance_thresh) or \n",
    "        points_are_close(line1[1], line2[1], distance_thresh) or \n",
    "        points_are_close(line1[0], line2[1], distance_thresh) or \n",
    "        points_are_close(line1[1], line2[0], distance_thresh)):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to combine two lines\n",
    "def combine_lines(line1, line2):\n",
    "    points = [line1[0], line1[1], line2[0], line2[1]]\n",
    "    \n",
    "    # Initialize variables to track the maximum distance and the corresponding points\n",
    "    max_dist = 0\n",
    "    start_point = points[0]\n",
    "    end_point = points[1]\n",
    "    \n",
    "    # Iterate over all pairs of points to find the pair with the maximum distance\n",
    "    for i in range(len(points)):\n",
    "        for j in range(i + 1, len(points)):\n",
    "            dist = distance(points[i], points[j])\n",
    "            if dist > max_dist:\n",
    "                max_dist = dist\n",
    "                start_point = points[i]\n",
    "                end_point = points[j]\n",
    "    \n",
    "    # Return the start and end points that form the combined line\n",
    "    return (start_point, end_point)\n",
    "\n",
    "# Function to process a single temp image and generate combined lines and labels\n",
    "def process_file(temp_image_path, json_file_path):\n",
    "    img = cv2.imread(temp_image_path)\n",
    "    if img is None:\n",
    "        print(f\"[ERROR] Image not found: {temp_image_path}\")\n",
    "        return None\n",
    "\n",
    "    # Extract the base name for saving results\n",
    "    base_name = os.path.splitext(os.path.basename(temp_image_path))[0]\n",
    "\n",
    "    # Read the detected lines from the JSON file\n",
    "    with open(json_file_path, \"r\") as f:\n",
    "        line_data = json.load(f)\n",
    "    \n",
    "    # Extract lines from the JSON file\n",
    "    line_points = [(tuple(line[\"start\"]), tuple(line[\"end\"])) for line in line_data[\"lines\"]]\n",
    "\n",
    "    # Create images for visualization\n",
    "    line_img = img.copy()\n",
    "\n",
    "    # Store detected lines (draw them on the debug image)\n",
    "    for idx, (start, end) in enumerate(line_points):\n",
    "        x1, y1 = start\n",
    "        x2, y2 = end\n",
    "        cv2.line(line_img, (x1, y1), (x2, y2), (0, 255, 255), 2)  # Draw detected lines\n",
    "        # Annotate line number\n",
    "        mid_x, mid_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "        cv2.putText(line_img, f'{idx+1}', (mid_x, mid_y), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "    # Combine lines based on proximity\n",
    "    combined_lines = []\n",
    "    for line1 in line_points:\n",
    "        merged = False\n",
    "        for i, line2 in enumerate(combined_lines):\n",
    "            if should_combine(line1, line2):\n",
    "                combined_lines[i] = combine_lines(line1, line2)\n",
    "                merged = True\n",
    "                break\n",
    "        if not merged:\n",
    "            combined_lines.append(line1)\n",
    "\n",
    "    # Create a copy of the cleaned image to draw combined lines and labels\n",
    "    line_img_combined = img.copy()\n",
    "\n",
    "    # Draw combined lines and add labels\n",
    "    for idx, line in enumerate(combined_lines):\n",
    "        x1, y1 = line[0]\n",
    "        x2, y2 = line[1]\n",
    "        cv2.line(line_img_combined, (x1, y1), (x2, y2), (255, 255, 0), 2)  # Draw blue lines for combined lines\n",
    "        # Get midpoint to place label\n",
    "        mid_x, mid_y = midpoint((x1, y1), (x2, y2))\n",
    "        # Label the line with its number\n",
    "        cv2.putText(line_img_combined, f'{idx+1}', (mid_x, mid_y), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2)\n",
    "\n",
    "    # Save the visualized image for combined lines\n",
    "    combined_path = os.path.join(os.path.dirname(temp_image_path), f\"{base_name}_combined_lines.png\")\n",
    "    cv2.imwrite(combined_path, line_img_combined)\n",
    "\n",
    "    # Display the image for combined lines\n",
    "    plt.figure(figsize=(96, 48))\n",
    "    plt.title(f\"Combined Lines for {base_name}\")\n",
    "    plt.imshow(cv2.cvtColor(line_img_combined, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Output the start and end points of the combined lines with labels\n",
    "    print(f\"Detected {len(combined_lines)} combined lines.\")\n",
    "    for idx, points in enumerate(combined_lines):\n",
    "        print(f\"Combined Line {idx+1}: Start {points[0]}, End {points[1]}\")\n",
    "\n",
    "# Function to process all temp images (temp_files) and their respective JSON files\n",
    "def process_files(temp_files, json_files):\n",
    "    \"\"\"Process multiple temp image files for line detection and combination.\"\"\"\n",
    "    if not temp_files or not json_files:\n",
    "        print(\"[INFO] No files to process.\")\n",
    "        return\n",
    "\n",
    "    for temp_image_path, json_file_path in zip(temp_files, json_files):\n",
    "        if not os.path.exists(temp_image_path):\n",
    "            print(f\"[ERROR] File not found: {temp_image_path}\")\n",
    "            continue\n",
    "        if not os.path.exists(json_file_path):\n",
    "            print(f\"[ERROR] JSON file not found: {json_file_path}\")\n",
    "            continue\n",
    "        process_file(temp_image_path, json_file_path)\n",
    "\n",
    "    print(\"[INFO] Line detection and combination completed for all files.\")\n",
    "\n",
    "# Ensure this part is executed to process multiple files\n",
    "if 'temp_files' in globals() and 'json_files' in globals():\n",
    "    process_files(temp_files, json_files)\n",
    "else:\n",
    "    print(\"[ERROR] 'temp_files' or 'json_files' not found. Please provide valid lists of temp image paths and JSON files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45f60c-2a44-4af5-b8c5-ad3d81bfb549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def find_processed_files_directory(file_paths):\n",
    "    \"\"\"\n",
    "    Determines the location of the 'processed_files' directory based on the parent folder of the first file in the list.\n",
    "    Assumes 'processed_files' is at the same level as the parent folder of the input files.\n",
    "    \"\"\"\n",
    "    if not file_paths:\n",
    "        print(\"[ERROR] No files provided to locate 'processed_files' directory.\")\n",
    "        return None\n",
    "\n",
    "    # Get the parent directory of the first file's directory\n",
    "    parent_folder = os.path.dirname(os.path.dirname(file_paths[0]))\n",
    "    processed_files_path = os.path.join(parent_folder, \"processed_files\")\n",
    "\n",
    "    # Check if the directory exists\n",
    "    if os.path.exists(processed_files_path):\n",
    "        return processed_files_path\n",
    "\n",
    "    print(\"[ERROR] 'processed_files' directory not found.\")\n",
    "    return None\n",
    "\n",
    "def filter_and_update_rectangles_for_files(file_paths):\n",
    "    \"\"\"\n",
    "    Iterates over each file in `files_to_process` and:\n",
    "      1) Filters out rectangles with OCR errors.\n",
    "      2) Updates the corresponding rectangle_text_mapping.json file dynamically.\n",
    "    \"\"\"\n",
    "    # Locate the 'processed_files' directory dynamically\n",
    "    processed_files_directory = find_processed_files_directory(file_paths)\n",
    "    if not processed_files_directory:\n",
    "        print(\"[ERROR] Unable to proceed without 'processed_files' directory.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Located 'processed_files' directory: {processed_files_directory}\")\n",
    "\n",
    "    for original_image_path in file_paths:\n",
    "        # Skip non-PNG files\n",
    "        if not original_image_path.lower().endswith('.png'):\n",
    "            print(f\"[INFO] Skipping unsupported file type: {original_image_path}\")\n",
    "            continue\n",
    "\n",
    "        base_filename = os.path.splitext(os.path.basename(original_image_path))[0]\n",
    "\n",
    "        # Dynamically construct the JSON file path\n",
    "        json_path = os.path.join(processed_files_directory, f\"{base_filename}_text.json\")\n",
    "\n",
    "        if not os.path.exists(json_path):\n",
    "            print(f\"[ERROR] JSON file not found: {json_path}. Skipping this file.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Processing rectangles for file: {json_path}\")\n",
    "\n",
    "        # Load the existing JSON data\n",
    "        with open(json_path, \"r\") as infile:\n",
    "            existing_data = json.load(infile)\n",
    "\n",
    "        # Ensure the rectangles and texts are defined\n",
    "        rectangles = existing_data.get(\"rectangles\", [])\n",
    "        if not rectangles:\n",
    "            print(f\"[INFO] No rectangles found in {json_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Filter out rectangles with OCR errors\n",
    "        filtered_rectangles = []\n",
    "        for text_data in rectangles:\n",
    "            if \"Error during OCR\" not in text_data[\"Text\"]:\n",
    "                filtered_rectangles.append(text_data)\n",
    "\n",
    "        # Display filtered rectangles and associated text\n",
    "        print(\"Filtered Rectangles and Texts:\")\n",
    "        for text_data in filtered_rectangles:\n",
    "            print(f\"Rectangle {text_data['Rectangle No']}: {text_data['Text']}\")\n",
    "\n",
    "        print(f\"Number of rectangles after filtering: {len(filtered_rectangles)}\")\n",
    "\n",
    "        # Update the JSON data\n",
    "        existing_data[\"rectangles\"] = filtered_rectangles\n",
    "\n",
    "        # Save the updated JSON file\n",
    "        with open(json_path, \"w\") as outfile:\n",
    "            json.dump(existing_data, outfile, indent=4)\n",
    "\n",
    "        print(f\"[SUCCESS] Filtered rectangle texts updated in '{json_path}'\\n\")\n",
    "\n",
    "# Example usage:\n",
    "filter_and_update_rectangles_for_files(files_to_process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22cb69c-243e-4542-a59f-5e80a5e4755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Debug function to check for 'rectangles' key in all JSON files stored in rectangle_json_files\n",
    "def debug_check_rectangle_json_files():\n",
    "    \"\"\"\n",
    "    Debug function to check the 'rectangles' key in all JSON files stored in rectangle_json_files.\n",
    "    \"\"\"\n",
    "    # Check if rectangle_json_files exists and is not empty\n",
    "    if 'rectangle_json_files' not in globals() or not rectangle_json_files:\n",
    "        print(\"[ERROR] 'rectangle_json_files' is not defined or is empty. Please run the processing step first.\")\n",
    "        return\n",
    "\n",
    "    print(\"[INFO] Starting debug for rectangle JSON files...\")\n",
    "    for json_file in rectangle_json_files:\n",
    "        # Ensure the file exists\n",
    "        if not os.path.exists(json_file):\n",
    "            print(f\"[ERROR] File not found: {json_file}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Inspecting file: {json_file}\")\n",
    "        try:\n",
    "            # Open and load JSON data\n",
    "            with open(json_file, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Check for 'rectangles' key\n",
    "            if \"rectangles\" in data:\n",
    "                print(f\"[SUCCESS] 'rectangles' key found in: {json_file}\")\n",
    "                print(f\"[INFO] Number of rectangles: {len(data['rectangles'])}\")\n",
    "                \n",
    "                # Print details of the first rectangle (optional)\n",
    "                if data[\"rectangles\"]:\n",
    "                    print(f\"[INFO] Example rectangle data: {data['rectangles'][0]}\")\n",
    "            else:\n",
    "                print(f\"[INFO] 'rectangles' key NOT found in: {json_file}\")\n",
    "                print(f\"[INFO] Available keys in this file: {list(data.keys())}\")\n",
    "        \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"[ERROR] Failed to decode JSON in file: {json_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Unexpected error processing {json_file}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if 'rectangle_json_files' in globals():\n",
    "    debug_check_rectangle_json_files()\n",
    "else:\n",
    "    print(\"[ERROR] 'rectangle_json_files' is not defined. Please ensure the JSON files are generated and stored.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6698d6e-9ecc-4c56-ae66-c8db3f0072e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def debug_environment():\n",
    "    \"\"\"\n",
    "    Debug function to validate the existence and contents of required variables and files.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    # Check if 'rectangle_json_files' is defined and populated\n",
    "    if 'rectangle_json_files' not in globals():\n",
    "        errors.append(\"[ERROR] 'rectangle_json_files' variable is not defined.\")\n",
    "    elif not rectangle_json_files:\n",
    "        errors.append(\"[ERROR] 'rectangle_json_files' is empty. Please ensure rectangle data is processed.\")\n",
    "    else:\n",
    "        print(\"[INFO] 'rectangle_json_files' is defined and contains the following files:\")\n",
    "        for file in rectangle_json_files:\n",
    "            if os.path.exists(file):\n",
    "                print(f\"[INFO] Found: {file}\")\n",
    "            else:\n",
    "                errors.append(f\"[ERROR] File not found: {file}\")\n",
    "\n",
    "    # Check if 'json_files' is defined and populated (for line coordinates)\n",
    "    if 'json_files' not in globals():\n",
    "        errors.append(\"[ERROR] 'json_files' variable is not defined.\")\n",
    "    elif not json_files:\n",
    "        errors.append(\"[ERROR] 'json_files' is empty. Please ensure line data is processed.\")\n",
    "    else:\n",
    "        print(\"[INFO] 'json_files' is defined and contains the following files:\")\n",
    "        for file in json_files:\n",
    "            if os.path.exists(file):\n",
    "                print(f\"[INFO] Found: {file}\")\n",
    "            else:\n",
    "                errors.append(f\"[ERROR] File not found: {file}\")\n",
    "\n",
    "    # Summary of errors\n",
    "    if errors:\n",
    "        print(\"\\n\".join(errors))\n",
    "    else:\n",
    "        print(\"[SUCCESS] All required variables and files are correctly defined and exist.\")\n",
    "\n",
    "def validate_json_files(json_file_list, required_keys):\n",
    "    \"\"\"\n",
    "    Validate the contents of a list of JSON files to ensure they contain required keys.\n",
    "    \"\"\"\n",
    "    for file in json_file_list:\n",
    "        if not os.path.exists(file):\n",
    "            print(f\"[ERROR] File not found: {file}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(file, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            missing_keys = [key for key in required_keys if key not in data]\n",
    "            if missing_keys:\n",
    "                print(f\"[ERROR] Missing keys {missing_keys} in file: {file}\")\n",
    "            else:\n",
    "                print(f\"[INFO] File {file} contains all required keys: {required_keys}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"[ERROR] Failed to decode JSON in file: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Unexpected error while reading {file}: {e}\")\n",
    "\n",
    "# Run the debug environment check\n",
    "print(\"[DEBUG] Validating environment...\")\n",
    "debug_environment()\n",
    "\n",
    "# Validate rectangle JSON files\n",
    "if 'rectangle_json_files' in globals() and rectangle_json_files:\n",
    "    print(\"\\n[DEBUG] Validating rectangle JSON files for required keys...\")\n",
    "    validate_json_files(rectangle_json_files, required_keys=[\"rectangles\"])\n",
    "\n",
    "# Validate line JSON files\n",
    "if 'json_files' in globals() and json_files:\n",
    "    print(\"\\n[DEBUG] Validating line JSON files for required keys...\")\n",
    "    validate_json_files(json_files, required_keys=[\"combined_lines\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e19993-6096-499c-9df1-ea448f53df60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Global variable to store adjacency matrix file paths\n",
    "adjacency_matrix_files = []\n",
    "\n",
    "def point_to_line_distance(line_start, line_end, point):\n",
    "    \"\"\"\n",
    "    Calculate the distance from a point to a line segment.\n",
    "    \"\"\"\n",
    "    line_start = np.array(line_start).reshape(2)\n",
    "    line_end = np.array(line_end).reshape(2)\n",
    "    point = np.array(point).reshape(2)\n",
    "\n",
    "    line_vec = line_end - line_start\n",
    "    point_vec = point - line_start\n",
    "\n",
    "    line_len = np.linalg.norm(line_vec)\n",
    "    if line_len == 0:\n",
    "        return np.linalg.norm(point_vec)  # Handle degenerate line (start == end)\n",
    "\n",
    "    line_unitvec = line_vec / line_len\n",
    "    projection_length = np.dot(point_vec, line_unitvec)\n",
    "\n",
    "    if projection_length < 0:\n",
    "        closest_point = line_start\n",
    "    elif projection_length > line_len:\n",
    "        closest_point = line_end\n",
    "    else:\n",
    "        closest_point = line_start + projection_length * line_unitvec\n",
    "\n",
    "    return np.linalg.norm(point - closest_point)\n",
    "\n",
    "def is_point_close_to_rectangle(point, rectangle, threshold):\n",
    "    \"\"\"\n",
    "    Check if a point is close to any edge of a rectangle.\n",
    "    \"\"\"\n",
    "    if len(rectangle) != 4:\n",
    "        raise ValueError(\"Rectangle must be a list of 4 points.\")\n",
    "\n",
    "    edges = [\n",
    "        (rectangle[0], rectangle[1]),\n",
    "        (rectangle[1], rectangle[2]),\n",
    "        (rectangle[2], rectangle[3]),\n",
    "        (rectangle[3], rectangle[0])\n",
    "    ]\n",
    "\n",
    "    for edge_start, edge_end in edges:\n",
    "        if point_to_line_distance(edge_start, edge_end, point) < threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def compute_adjacency_matrix_via_lines(rectangle_json_path, line_json_path, threshold=30):\n",
    "    \"\"\"\n",
    "    Computes adjacency matrix for rectangles based on proximity to line endpoints\n",
    "    and saves it to a JSON file.\n",
    "    \"\"\"\n",
    "    global adjacency_matrix_files  # Use global to store paths dynamically\n",
    "\n",
    "    # Load rectangle data\n",
    "    with open(rectangle_json_path, \"r\") as rect_file:\n",
    "        rectangle_data = json.load(rect_file)\n",
    "    \n",
    "    rectangles = rectangle_data.get(\"rectangles\", [])\n",
    "    if not rectangles:\n",
    "        print(f\"[ERROR] No 'rectangles' key found in {rectangle_json_path}.\")\n",
    "        return\n",
    "\n",
    "    rectangle_coords = [rect[\"Coordinates\"] for rect in rectangles if rect[\"Coordinates\"]]\n",
    "    num_rectangles = len(rectangle_coords)\n",
    "\n",
    "    if num_rectangles == 0:\n",
    "        print(f\"[ERROR] No valid rectangle coordinates found in {rectangle_json_path}.\")\n",
    "        return\n",
    "\n",
    "    # Load line data\n",
    "    with open(line_json_path, \"r\") as line_file:\n",
    "        line_data = json.load(line_file)\n",
    "\n",
    "    lines = line_data.get(\"combined_lines\", [])\n",
    "    if not lines:\n",
    "        print(f\"[ERROR] No 'combined_lines' key found in {line_json_path}.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Number of rectangles: {num_rectangles}\")\n",
    "    print(f\"[INFO] Number of lines: {len(lines)}\")\n",
    "\n",
    "    # Initialize adjacency matrix\n",
    "    adjacency_matrix = np.zeros((num_rectangles, num_rectangles), dtype=bool)\n",
    "\n",
    "    # Check each line's proximity to rectangles\n",
    "    for line in lines:\n",
    "        line_start, line_end = line[\"start\"], line[\"end\"]\n",
    "        start_close_to_rect = -1\n",
    "        end_close_to_rect = -1\n",
    "\n",
    "        # Check if line start is close to any rectangle\n",
    "        for i, rectangle in enumerate(rectangle_coords):\n",
    "            try:\n",
    "                if is_point_close_to_rectangle(line_start, rectangle, threshold):\n",
    "                    start_close_to_rect = i\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping invalid rectangle at index {i}: {e}\")\n",
    "\n",
    "        # Check if line end is close to any rectangle\n",
    "        for i, rectangle in enumerate(rectangle_coords):\n",
    "            try:\n",
    "                if is_point_close_to_rectangle(line_end, rectangle, threshold):\n",
    "                    end_close_to_rect = i\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping invalid rectangle at index {i}: {e}\")\n",
    "\n",
    "        # Mark adjacency if line connects two different rectangles\n",
    "        if start_close_to_rect != -1 and end_close_to_rect != -1 and start_close_to_rect != end_close_to_rect:\n",
    "            adjacency_matrix[start_close_to_rect, end_close_to_rect] = True\n",
    "            adjacency_matrix[end_close_to_rect, start_close_to_rect] = True  # Symmetrical adjacency\n",
    "\n",
    "    # Save adjacency matrix to a JSON file\n",
    "    adjacency_output_path = os.path.join(\n",
    "        os.path.dirname(rectangle_json_path),\n",
    "        f\"{os.path.splitext(os.path.basename(rectangle_json_path))[0]}_adjacency_matrix.json\"\n",
    "    )\n",
    "    with open(adjacency_output_path, \"w\") as outfile:\n",
    "        json.dump(adjacency_matrix.tolist(), outfile, indent=4)\n",
    "\n",
    "    print(f\"[SUCCESS] Adjacency matrix saved to: {adjacency_output_path}\\n\")\n",
    "\n",
    "    # Add the adjacency matrix file path to the global list\n",
    "    adjacency_matrix_files.append(adjacency_output_path)\n",
    "\n",
    "def compute_adjacency_matrices_via_lines(rectangle_json_files, line_json_files, threshold=30):\n",
    "    \"\"\"\n",
    "    Computes adjacency matrices for multiple JSON files containing rectangle data and lines.\n",
    "    \"\"\"\n",
    "    if not rectangle_json_files or not line_json_files:\n",
    "        print(\"[ERROR] Either 'rectangle_json_files' or 'line_json_files' is empty.\")\n",
    "        return\n",
    "\n",
    "    # Ensure equal pairing of rectangle and line files\n",
    "    if len(rectangle_json_files) != len(line_json_files):\n",
    "        print(f\"[ERROR] Mismatch in number of rectangle and line files.\")\n",
    "        print(f\"Rectangle files: {len(rectangle_json_files)}, Line files: {len(line_json_files)}\")\n",
    "        return\n",
    "\n",
    "    for rect_file, line_file in zip(rectangle_json_files, line_json_files):\n",
    "        if not os.path.exists(rect_file):\n",
    "            print(f\"[ERROR] Rectangle file not found: {rect_file}. Skipping.\")\n",
    "            continue\n",
    "        if not os.path.exists(line_file):\n",
    "            print(f\"[ERROR] Line file not found: {line_file}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Processing rectangle file: {rect_file} with line file: {line_file}\")\n",
    "        compute_adjacency_matrix_via_lines(rect_file, line_file, threshold=threshold)\n",
    "\n",
    "    # Display all collected adjacency matrix files\n",
    "    print(\"\\n[INFO] All generated adjacency matrix files:\")\n",
    "    for file in adjacency_matrix_files:\n",
    "        print(f\" - {file}\")\n",
    "\n",
    "# Usage Example\n",
    "if 'rectangle_json_files' in globals() and rectangle_json_files and 'json_files' in globals() and json_files:\n",
    "    print(\"[INFO] Using dynamically generated rectangle and line JSON files.\")\n",
    "    compute_adjacency_matrices_via_lines(rectangle_json_files, json_files, threshold=30)\n",
    "else:\n",
    "    print(\"[ERROR] Required JSON files or variables are missing. Ensure all data is generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bda1d9f-77b8-4f54-8ff5-645a6c460e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "def process_connections_for_files(rectangle_json_files, adjacency_matrix_files):\n",
    "    \"\"\"\n",
    "    Processes multiple files to generate connections based on adjacency matrices\n",
    "    and rectangle indices.\n",
    "    \"\"\"\n",
    "    if len(rectangle_json_files) != len(adjacency_matrix_files):\n",
    "        raise ValueError(\"Mismatch between the number of rectangle JSON files and adjacency matrix files.\")\n",
    "\n",
    "    all_connections = {}  # Store connections for all files\n",
    "\n",
    "    for rect_file, adj_file in zip(rectangle_json_files, adjacency_matrix_files):\n",
    "        # Validate files exist\n",
    "        if not os.path.exists(rect_file):\n",
    "            print(f\"[ERROR] Rectangle file not found: {rect_file}. Skipping.\")\n",
    "            continue\n",
    "        if not os.path.exists(adj_file):\n",
    "            print(f\"[ERROR] Adjacency matrix file not found: {adj_file}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load rectangles\n",
    "        with open(rect_file, \"r\") as f:\n",
    "            rectangle_data = json.load(f)\n",
    "        rectangles = rectangle_data.get(\"rectangles\", [])\n",
    "        num_rectangles = len(rectangles)\n",
    "\n",
    "        if num_rectangles == 0:\n",
    "            print(f\"[ERROR] No rectangles found in file: {rect_file}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load adjacency matrix\n",
    "        with open(adj_file, \"r\") as f:\n",
    "            adjacency_matrix = np.array(json.load(f), dtype=bool)\n",
    "\n",
    "        # Validate adjacency matrix size\n",
    "        if adjacency_matrix.shape[0] != num_rectangles:\n",
    "            print(f\"[ERROR] Mismatch between number of rectangles and adjacency matrix size for file: {rect_file}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Initialize connections dictionary for the current file\n",
    "        connections = {}\n",
    "\n",
    "        # Extract edges from the adjacency matrix\n",
    "        for i in range(num_rectangles):\n",
    "            source_label = f\"Rectangle {i+1}\"\n",
    "            if source_label not in connections:\n",
    "                connections[source_label] = []  # Initialize an empty list for connections\n",
    "\n",
    "            for j in range(num_rectangles):\n",
    "                if adjacency_matrix[i, j]:  # If there's a connection between rectangle i and j\n",
    "                    target_label = f\"Rectangle {j+1}\"\n",
    "                    connections[source_label].append(target_label)\n",
    "\n",
    "        # Add connections for the current file to the global dictionary\n",
    "        file_key = os.path.basename(rect_file).replace(\"_text.json\", \"\")\n",
    "        all_connections[file_key] = connections\n",
    "\n",
    "    # Display all connections\n",
    "    print(\"\\nList of Connections for All Files:\")\n",
    "    for file, connections in all_connections.items():\n",
    "        print(f\"\\nConnections for {file}:\")\n",
    "        for source, targets in connections.items():\n",
    "            if targets:\n",
    "                targets_str = ', '.join(targets)\n",
    "                print(f\"  {source} is linked with: {targets_str}\")\n",
    "            else:\n",
    "                print(f\"  {source} has no connections.\")\n",
    "\n",
    "    return all_connections  # Return the aggregated connections for further use\n",
    "\n",
    "# Example Usage\n",
    "if 'rectangle_json_files' in globals() and 'adjacency_matrix_files' in globals():\n",
    "    connections = process_connections_for_files(rectangle_json_files, adjacency_matrix_files)\n",
    "else:\n",
    "    print(\"[ERROR] Required files (rectangle_json_files or adjacency_matrix_files) are not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae23f2-2f35-4732-abd7-de9a98335265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "def plot_adjacency_matrix_with_labels(adjacency_matrix, labels, title):\n",
    "    \"\"\"\n",
    "    Plot the adjacency matrix with custom labels and save/display it.\n",
    "    \"\"\"\n",
    "    num_rectangles = adjacency_matrix.shape[0]\n",
    "    \n",
    "    # Validate that the number of labels matches the adjacency matrix size\n",
    "    if num_rectangles != len(labels):\n",
    "        raise ValueError(\"Number of labels does not match the dimensions of the adjacency matrix.\")\n",
    "    \n",
    "    plt.figure(figsize=(120, 120))  # Increased the figure size\n",
    "\n",
    "    # Create the matrix plot\n",
    "    plt.matshow(adjacency_matrix, cmap='Blues', fignum=1)\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title(title, pad=80, fontsize=80)\n",
    "    plt.xlabel('Rectangles', labelpad=50, fontsize=60)\n",
    "    plt.ylabel('Rectangles', labelpad=50, fontsize=60)\n",
    "\n",
    "    # Add custom labels to x and y axes\n",
    "    plt.xticks(ticks=np.arange(num_rectangles), labels=labels, rotation=90, ha='center', fontsize=50)\n",
    "    plt.yticks(ticks=np.arange(num_rectangles), labels=labels, fontsize=50)\n",
    "\n",
    "    # Add gridlines\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Add colorbar with adjusted font size\n",
    "    cbar = plt.colorbar(label='Connection (0=No, 1=Yes)')\n",
    "    cbar.ax.tick_params(labelsize=40)\n",
    "    cbar.set_label('Connection (0=No, 1=Yes)', fontsize=50)\n",
    "\n",
    "    # Annotate cells with values, using white text for darker cells\n",
    "    for (i, j), value in np.ndenumerate(adjacency_matrix):\n",
    "        color = 'white' if value == 1 else 'black'\n",
    "        plt.text(j, i, f'{value:.0f}', ha='center', va='center', color=color, fontsize=40)\n",
    "\n",
    "    # Save the plot\n",
    "    plot_filename = f\"{title.replace(' ', '_').lower()}_adjacency_matrix.png\"\n",
    "    plt.savefig(plot_filename, bbox_inches='tight')\n",
    "    print(f\"[INFO] Adjacency matrix plot saved as: {plot_filename}\")\n",
    "    \n",
    "    # Show the plot in the notebook\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def process_and_plot_adjacency_matrices(rectangle_json_files, adjacency_matrix_files):\n",
    "    \"\"\"\n",
    "    Process all adjacency matrix files and their corresponding rectangle labels to plot.\n",
    "    \"\"\"\n",
    "    if len(rectangle_json_files) != len(adjacency_matrix_files):\n",
    "        print(\"[ERROR] Mismatch in the number of rectangle and adjacency matrix files.\")\n",
    "        return\n",
    "\n",
    "    for rect_file, adj_file in zip(rectangle_json_files, adjacency_matrix_files):\n",
    "        # Load rectangle labels\n",
    "        if not os.path.exists(rect_file):\n",
    "            print(f\"[ERROR] Rectangle JSON file not found: {rect_file}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        with open(rect_file, \"r\") as f:\n",
    "            rectangle_data = json.load(f)\n",
    "        \n",
    "        rectangles = rectangle_data.get(\"rectangles\", [])\n",
    "        if not rectangles:\n",
    "            print(f\"[ERROR] No 'rectangles' key found in {rect_file}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        labels = [f\"Rectangle {i + 1}\" for i in range(len(rectangles))]\n",
    "\n",
    "        # Load adjacency matrix\n",
    "        if not os.path.exists(adj_file):\n",
    "            print(f\"[ERROR] Adjacency matrix file not found: {adj_file}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        with open(adj_file, \"r\") as f:\n",
    "            adjacency_matrix = np.array(json.load(f))\n",
    "\n",
    "        if adjacency_matrix.shape[0] != len(labels):\n",
    "            print(f\"[ERROR] Mismatch between adjacency matrix size and number of labels for {adj_file}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Generate title from rectangle file name\n",
    "        title = os.path.basename(rect_file).replace(\"_text.json\", \"\").replace(\"_\", \" \").title()\n",
    "\n",
    "        # Plot adjacency matrix\n",
    "        plot_adjacency_matrix_with_labels(adjacency_matrix, labels, title)\n",
    "\n",
    "# Usage Example\n",
    "if 'rectangle_json_files' in globals() and 'adjacency_matrix_files' in globals():\n",
    "    print(\"[INFO] Plotting adjacency matrices for all files...\")\n",
    "    process_and_plot_adjacency_matrices(rectangle_json_files, adjacency_matrix_files)\n",
    "else:\n",
    "    print(\"[ERROR] Required variables (rectangle_json_files or adjacency_matrix_files) are missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21854eb-e1d8-4e1f-b59f-5ffa0ec52a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Initialize a global list to store GraphML file locations\n",
    "graphml_files = []\n",
    "\n",
    "def generate_graphml(rectangle_json_file, adjacency_matrix_file, output_directory=\"graphml_files\"):\n",
    "    \"\"\"\n",
    "    Generate a GraphML file for a single pair of rectangle JSON and adjacency matrix file.\n",
    "    \"\"\"\n",
    "    # Load rectangle text mapping JSON\n",
    "    if not os.path.exists(rectangle_json_file):\n",
    "        print(f\"[ERROR] Rectangle JSON file not found: {rectangle_json_file}. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    with open(rectangle_json_file, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Create a dictionary mapping rectangle numbers to their summarised text\n",
    "    rectangle_text_map = {str(item[\"Rectangle No\"]): item[\"Text\"] for item in data[\"rectangles\"]}\n",
    "\n",
    "    # Load the adjacency matrix\n",
    "    if not os.path.exists(adjacency_matrix_file):\n",
    "        print(f\"[ERROR] Adjacency matrix file not found: {adjacency_matrix_file}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    with open(adjacency_matrix_file, \"r\") as file:\n",
    "        adjacency_matrix = np.array(json.load(file))\n",
    "\n",
    "    # Initialize a graph\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    # Add nodes with attributes (rectangle number and text)\n",
    "    for rect_no, text in rectangle_text_map.items():\n",
    "        graph.add_node(rect_no, label=f\"Rectangle {rect_no}\", text=text)\n",
    "\n",
    "    # Add edges based on the adjacency matrix\n",
    "    num_rectangles = adjacency_matrix.shape[0]\n",
    "    for i in range(num_rectangles):\n",
    "        for j in range(num_rectangles):\n",
    "            if adjacency_matrix[i, j]:  # If there is a connection\n",
    "                graph.add_edge(str(i + 1), str(j + 1))  # Rectangle numbers start from 1\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Generate GraphML file name based on rectangle JSON file\n",
    "    graphml_filename = os.path.join(\n",
    "        output_directory, f\"{os.path.splitext(os.path.basename(rectangle_json_file))[0]}_graph.graphml\"\n",
    "    )\n",
    "    \n",
    "    # Save the graph to a GraphML file\n",
    "    nx.write_graphml(graph, graphml_filename)\n",
    "    print(f\"[SUCCESS] GraphML file created: {graphml_filename}\")\n",
    "\n",
    "    # Append the file location to the global graphml_files list\n",
    "    graphml_files.append(graphml_filename)\n",
    "\n",
    "def generate_graphml_files(rectangle_json_files, adjacency_matrix_files, output_directory=\"graphml_files\"):\n",
    "    \"\"\"\n",
    "    Generate GraphML files for all rectangle JSON and adjacency matrix file pairs.\n",
    "    \"\"\"\n",
    "    if not rectangle_json_files or not adjacency_matrix_files:\n",
    "        print(\"[ERROR] Either 'rectangle_json_files' or 'adjacency_matrix_files' is empty.\")\n",
    "        return\n",
    "\n",
    "    if len(rectangle_json_files) != len(adjacency_matrix_files):\n",
    "        print(f\"[ERROR] Mismatch in the number of rectangle JSON files and adjacency matrix files.\")\n",
    "        print(f\"Rectangle files: {len(rectangle_json_files)}, Adjacency matrix files: {len(adjacency_matrix_files)}\")\n",
    "        return\n",
    "\n",
    "    for rect_file, adj_file in zip(rectangle_json_files, adjacency_matrix_files):\n",
    "        print(f\"[INFO] Generating GraphML for: {rect_file} and {adj_file}\")\n",
    "        generate_graphml(rect_file, adj_file, output_directory)\n",
    "\n",
    "    # Print the collected GraphML file locations\n",
    "    print(\"[INFO] All generated GraphML files:\")\n",
    "    for graphml_file in graphml_files:\n",
    "        print(f\" - {graphml_file}\")\n",
    "\n",
    "# Usage Example\n",
    "if 'rectangle_json_files' in globals() and 'adjacency_matrix_files' in globals():\n",
    "    print(\"[INFO] Generating GraphML files for all processed files...\")\n",
    "    generate_graphml_files(rectangle_json_files, adjacency_matrix_files)\n",
    "else:\n",
    "    print(\"[ERROR] Required variables (rectangle_json_files or adjacency_matrix_files) are missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3654a4f-82f2-4a01-8af7-43dd23f61f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae2616-0b19-45a7-909e-eb88dba6da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms.community import girvan_newman, louvain_communities\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Global variables to store results\n",
    "community_results_global = {}\n",
    "community_summaries = {}\n",
    "labels = {}\n",
    "\n",
    "# Load rectangle text mapping\n",
    "def load_rectangle_text_mapping(rectangle_json_file):\n",
    "    with open(rectangle_json_file, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return {str(item[\"Rectangle No\"]): item[\"Text\"] for item in data[\"rectangles\"]}\n",
    "\n",
    "# Girvan-Newman community detection\n",
    "def girvan_newman_communities(G, depth=1):\n",
    "    communities_generator = girvan_newman(G)\n",
    "    for _ in range(depth):\n",
    "        try:\n",
    "            communities = next(communities_generator)\n",
    "        except StopIteration:\n",
    "            print(\"[INFO] Reached the end of community splits.\")\n",
    "            return []\n",
    "    return [list(community) for community in communities]\n",
    "\n",
    "# Louvain community detection\n",
    "def louvain_community_detection(G):\n",
    "    return list(louvain_communities(G))\n",
    "\n",
    "# Calculate modularity\n",
    "def calculate_modularity(G, communities):\n",
    "    return modularity(G, communities)\n",
    "\n",
    "# Visualise graph with communities\n",
    "def draw_graph_with_communities(G, communities_gn, communities_lv, node_labels, output_directory, output_filename):\n",
    "    pos = nx.spring_layout(G, seed=42, k=0.6)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(96, 64))\n",
    "\n",
    "    colors_gn = plt.get_cmap('tab20', len(communities_gn))\n",
    "    colors_lv = plt.get_cmap('tab20', len(communities_lv))\n",
    "\n",
    "    # Girvan-Newman\n",
    "    axes[0].set_title('Girvan-Newman Communities', fontsize=48, fontweight='bold')\n",
    "    for i, comm in enumerate(communities_gn):\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=comm, ax=axes[0], node_color=[colors_gn(i)], node_size=7000, alpha=0.9, label=f'Community {i + 1}')\n",
    "    nx.draw_networkx_edges(G, pos, ax=axes[0], alpha=0.7, edge_color=\"gray\", width=5)\n",
    "    nx.draw_networkx_labels(G, pos, labels=node_labels, ax=axes[0], font_size=24, font_color=\"black\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    handles_gn, labels_gn = axes[0].get_legend_handles_labels()\n",
    "    axes[0].legend(handles_gn, labels_gn, loc='best', fontsize=36)\n",
    "\n",
    "    # Louvain\n",
    "    axes[1].set_title('Louvain Communities', fontsize=48, fontweight='bold')\n",
    "    for i, comm in enumerate(communities_lv):\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=comm, ax=axes[1], node_color=[colors_lv(i)], node_size=7000, alpha=0.9, label=f'Community {i + 1}')\n",
    "    nx.draw_networkx_edges(G, pos, ax=axes[1], alpha=0.7, edge_color=\"gray\", width=5)\n",
    "    nx.draw_networkx_labels(G, pos, labels=node_labels, ax=axes[1], font_size=24, font_color=\"black\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    handles_lv, labels_lv = axes[1].get_legend_handles_labels()\n",
    "    axes[1].legend(handles_lv, labels_lv, loc='best', fontsize=36)\n",
    "\n",
    "    plt.suptitle('Graph with Community Detection', fontsize=64, fontweight='bold')\n",
    "\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    output_path = os.path.join(output_directory, f\"{output_filename}_communities.png\")\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"[SUCCESS] Community plot saved to: {output_path}\")\n",
    "\n",
    "    return output_path\n",
    "\n",
    "# Display results in a table\n",
    "def display_results_as_table(community_results):\n",
    "    df = pd.DataFrame([community_results])\n",
    "    print(\"\\n[INFO] Community Detection Results:\")\n",
    "    print(df)\n",
    "\n",
    "# Process a single graph\n",
    "def process_graph(graphml_file, rectangle_json_file, output_directory=\"community_results\", depth=1):\n",
    "    if not os.path.exists(graphml_file):\n",
    "        print(f\"[ERROR] GraphML file not found: {graphml_file}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(rectangle_json_file):\n",
    "        print(f\"[ERROR] Rectangle JSON file not found: {rectangle_json_file}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Load graph and rectangle data\n",
    "    graph = nx.read_graphml(graphml_file, node_type=str)\n",
    "    rectangle_text_map = load_rectangle_text_mapping(rectangle_json_file)\n",
    "    node_labels = {node: rectangle_text_map.get(node, f\"Node {node}\") for node in graph.nodes}\n",
    "\n",
    "    labels[graphml_file] = list(node_labels.values())\n",
    "\n",
    "    # Detect communities\n",
    "    communities_gn = girvan_newman_communities(graph, depth=depth)\n",
    "    communities_lv = louvain_community_detection(graph)\n",
    "\n",
    "    if not communities_gn or not communities_lv:\n",
    "        print(f\"[WARNING] No communities detected in {graphml_file}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Visualise and save graph\n",
    "    output_filename = os.path.splitext(os.path.basename(graphml_file))[0]\n",
    "    plot_path = draw_graph_with_communities(graph, communities_gn, communities_lv, node_labels, output_directory, output_filename)\n",
    "\n",
    "    # Calculate modularity\n",
    "    girvan_mod = calculate_modularity(graph, communities_gn)\n",
    "    louvain_mod = calculate_modularity(graph, communities_lv)\n",
    "\n",
    "    # Convert sets to lists for JSON serialization\n",
    "    communities_gn = [list(comm) for comm in communities_gn]\n",
    "    communities_lv = [list(comm) for comm in communities_lv]\n",
    "\n",
    "    # Store results\n",
    "    community_results = {\n",
    "        \"Girvan-Newman\": {\n",
    "            \"Communities\": communities_gn,  # Actual community nodes\n",
    "            \"Count\": len(communities_gn),\n",
    "            \"Modularity\": girvan_mod\n",
    "        },\n",
    "        \"Louvain\": {\n",
    "            \"Communities\": communities_lv,  # Actual community nodes\n",
    "            \"Count\": len(communities_lv),\n",
    "            \"Modularity\": louvain_mod\n",
    "        },\n",
    "        \"Plot Path\": plot_path\n",
    "    }\n",
    "    community_results_global[graphml_file] = community_results\n",
    "\n",
    "    summary = {\n",
    "        \"graphml_file\": graphml_file,\n",
    "        \"Girvan-Newman\": {\n",
    "            \"Count\": len(communities_gn),\n",
    "            \"Modularity\": girvan_mod\n",
    "        },\n",
    "        \"Louvain\": {\n",
    "            \"Count\": len(communities_lv),\n",
    "            \"Modularity\": louvain_mod\n",
    "        }\n",
    "    }\n",
    "    community_summaries[graphml_file] = summary\n",
    "\n",
    "    # Save individual results to a JSON file\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    json_output_file = os.path.join(output_directory, f\"{output_filename}_community_results.json\")\n",
    "    with open(json_output_file, \"w\") as outfile:\n",
    "        json.dump(community_results, outfile, indent=4)\n",
    "    print(f\"[SUCCESS] Community results saved to: {json_output_file}\")\n",
    "\n",
    "    # Display results in table format\n",
    "    display_results_as_table(community_results)\n",
    "\n",
    "    print(f\"[INFO] Processed {graphml_file} successfully.\\n\")\n",
    "\n",
    "# Process multiple files\n",
    "def process_multiple_files(graphml_files, rectangle_json_files, output_directory=\"community_results\", depth=1):\n",
    "    if not graphml_files or not rectangle_json_files:\n",
    "        print(\"[ERROR] GraphML or rectangle JSON files are missing.\")\n",
    "        return\n",
    "\n",
    "    if len(graphml_files) != len(rectangle_json_files):\n",
    "        print(f\"[ERROR] Mismatch in the number of GraphML and rectangle JSON files.\")\n",
    "        return\n",
    "\n",
    "    for graphml_file, rectangle_json_file in zip(graphml_files, rectangle_json_files):\n",
    "        print(f\"[INFO] Processing: {graphml_file} with {rectangle_json_file}\")\n",
    "        process_graph(graphml_file, rectangle_json_file, output_directory, depth)\n",
    "\n",
    "    # Print summary of results\n",
    "    print(\"\\n[INFO] Community Summaries for all files:\")\n",
    "    print(json.dumps(community_summaries, indent=4))\n",
    "\n",
    "# Usage Example\n",
    "if (\n",
    "    'graphml_files' in globals() and graphml_files and\n",
    "    'rectangle_json_files' in globals() and rectangle_json_files\n",
    "):\n",
    "    process_multiple_files(graphml_files, rectangle_json_files, output_directory=\"community_results\", depth=1)\n",
    "else:\n",
    "    print(\"[ERROR] Required variables (graphml_files or rectangle_json_files) are missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f95d5f7-a021-4699-bdb7-4d3d95aa1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92eeeb6-2fb8-42b4-8af7-3fc28ac6018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically load all GraphML files stored in the global list\n",
    "graphs = {}\n",
    "\n",
    "for graphml_file in graphml_files:\n",
    "    graph_name = os.path.splitext(os.path.basename(graphml_file))[0]\n",
    "    print(f\"[INFO] Loading GraphML: {graphml_file} as '{graph_name}'\")\n",
    "    graphs[graph_name] = nx.read_graphml(graphml_file, node_type=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4ff6c3-ca5a-44c0-b4a6-0bf7fe916f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rectangle_text_mapping(graph_name, processed_files_directory):\n",
    "    \"\"\"\n",
    "    Loads rectangle text mapping for a given graph name from the processed files directory.\n",
    "    Maps node numbers (like '1', '2', ...) to their text content.\n",
    "    \"\"\"\n",
    "    # Adjust file name to match earlier filename convention\n",
    "    base_name = graph_name.replace(\"_text_graph\", \"\")\n",
    "    json_path = os.path.join(processed_files_directory, f\"{base_name}_text.json\")\n",
    "\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"[ERROR] Rectangle text mapping file not found: {json_path}\")\n",
    "        return {}\n",
    "\n",
    "    with open(json_path, \"r\") as infile:\n",
    "        data = json.load(infile)\n",
    "\n",
    "    # Create a dictionary mapping node numbers to text content\n",
    "    return {\n",
    "        str(rect['Rectangle No']): rect['Text']\n",
    "        for rect in data.get(\"rectangles\", [])\n",
    "    }\n",
    "\n",
    "\n",
    "def girvan_newman_first_level_with_texts(G, rectangle_text_mapping):\n",
    "    \"\"\"\n",
    "    Runs the Girvan-Newman algorithm to find first-level communities and \n",
    "    replaces node identifiers with their corresponding text content.\n",
    "    Also returns valid node identifiers for modularity calculation.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Starting Girvan-Newman community detection for first-level communities...\")\n",
    "\n",
    "    try:\n",
    "        # Generate the first level of communities\n",
    "        communities_generator = girvan_newman(G)\n",
    "        first_level_communities = next(communities_generator)\n",
    "    except StopIteration:\n",
    "        print(\"[WARN] No communities could be detected.\")\n",
    "        return [], []\n",
    "\n",
    "    # Separate communities as node identifiers and mapped text content\n",
    "    communities_with_texts = []\n",
    "    communities_with_identifiers = []\n",
    "\n",
    "    for community in first_level_communities:\n",
    "        communities_with_texts.append(\n",
    "            [rectangle_text_mapping.get(node, f\"[Text not found for {node}]\") for node in community]\n",
    "        )\n",
    "        communities_with_identifiers.append(list(community))\n",
    "\n",
    "    return communities_with_identifiers, communities_with_texts\n",
    "\n",
    "\n",
    "def process_all_graphs_first_level_with_texts(graphs, processed_files_directory):\n",
    "    \"\"\"\n",
    "    Processes all graphs to detect first-level communities, replacing node identifiers with text content,\n",
    "    and also returning node identifiers for modularity calculations.\n",
    "    \"\"\"\n",
    "    all_communities = {}  # Store results by graph name\n",
    "\n",
    "    for graph_name, graph in graphs.items():\n",
    "        print(f\"\\n[INFO] Processing graph: {graph_name}\")\n",
    "\n",
    "        # Load rectangle text mapping\n",
    "        rectangle_text_mapping = load_rectangle_text_mapping(graph_name, processed_files_directory)\n",
    "        if not rectangle_text_mapping:\n",
    "            print(f\"[ERROR] Skipping graph '{graph_name}' due to missing text mapping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Run Girvan-Newman algorithm for first-level communities\n",
    "            communities_with_identifiers, communities_with_texts = girvan_newman_first_level_with_texts(graph, rectangle_text_mapping)\n",
    "            all_communities[graph_name] = {\n",
    "                \"file\": f\"{graph_name}.graphml\",\n",
    "                \"communities\": communities_with_texts\n",
    "            }\n",
    "\n",
    "            print(f\"[SUCCESS] First-level communities detected for '{graph_name}':\")\n",
    "            for idx, community in enumerate(communities_with_texts, start=1):\n",
    "                print(f\"  Community {idx}: {community}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process graph '{graph_name}': {e}\")\n",
    "\n",
    "    return all_communities\n",
    "\n",
    "\n",
    "# Locate the processed files directory dynamically\n",
    "processed_files_directory = find_processed_files_directory(graphml_files)\n",
    "\n",
    "# Run the function and save results\n",
    "all_graph_first_level_communities_with_texts = process_all_graphs_first_level_with_texts(graphs, processed_files_directory)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "output_file_with_texts = \"graph_first_level_communities_with_texts.json\"\n",
    "with open(output_file_with_texts, \"w\") as f:\n",
    "    json.dump(all_graph_first_level_communities_with_texts, f, indent=4)\n",
    "\n",
    "print(f\"\\n[INFO] First-level communities with text content have been saved to '{output_file_with_texts}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69026a4-fe5f-493d-b51d-c41869e13e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_modularity(G, communities):\n",
    "    \"\"\"\n",
    "    Calculates the modularity of a graph based on the detected communities.\n",
    "    \"\"\"\n",
    "    # Ensure communities are passed as sets of node identifiers\n",
    "    return modularity(G, [set(community) for community in communities])\n",
    "\n",
    "\n",
    "modularities = []\n",
    "for graph_name, graph in graphs.items():\n",
    "    print(f\"\\n[INFO] Processing modularity for graph: {graph_name}\")\n",
    "    \n",
    "    # Load rectangle text mapping\n",
    "    rectangle_text_mapping = load_rectangle_text_mapping(graph_name, processed_files_directory)\n",
    "    if not rectangle_text_mapping:\n",
    "        print(f\"[ERROR] Skipping graph '{graph_name}' due to missing text mapping.\")\n",
    "        continue\n",
    "\n",
    "    # Detect first-level communities and get node identifiers\n",
    "    communities_with_identifiers, _ = girvan_newman_first_level_with_texts(graph, rectangle_text_mapping)\n",
    "\n",
    "    # Convert to valid node identifier communities for modularity calculation\n",
    "    node_identifier_communities = [\n",
    "        set(community) for community in communities_with_identifiers if set(community).issubset(graph.nodes)\n",
    "    ]\n",
    "    \n",
    "    if node_identifier_communities:\n",
    "        # Calculate modularity based on the detected communities\n",
    "        mod = calculate_modularity(graph, node_identifier_communities)\n",
    "        modularities.append(mod)\n",
    "        print(f\"[SUCCESS] Modularity for {graph_name}: {mod:.4f}\")\n",
    "    else:\n",
    "        print(f\"[ERROR] Communities could not be found for '{graph_name}'.\")\n",
    "\n",
    "# Compute and display the average modularity\n",
    "if modularities:\n",
    "    average_modularity = sum(modularities) / len(modularities)\n",
    "    print(f\"\\n[INFO] Average Modularity: {average_modularity:.4f}\")\n",
    "else:\n",
    "    print(\"[WARN] No modularity values to average.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a94e14-82c4-4b9f-9095-054c95794682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_graph_community_information(graphs, processed_files_directory):\n",
    "    \"\"\"\n",
    "    Collects community information for each graph without displaying plots.\n",
    "    Returns a dictionary with graph names as keys and community information as values.\n",
    "    \"\"\"\n",
    "    all_graph_community_info = {}\n",
    "\n",
    "    for graph_name, graph in graphs.items():\n",
    "        print(f\"\\n[INFO] Processing community information for: {graph_name}\")\n",
    "\n",
    "        # Load rectangle text mapping\n",
    "        rectangle_text_mapping = load_rectangle_text_mapping(graph_name, processed_files_directory)\n",
    "\n",
    "        # Detect first-level communities\n",
    "        communities_with_identifiers, communities_with_texts = girvan_newman_first_level_with_texts(graph, rectangle_text_mapping)\n",
    "\n",
    "        if communities_with_identifiers:\n",
    "            # Store the information\n",
    "            all_graph_community_info[graph_name] = {\n",
    "                \"communities_with_identifiers\": communities_with_identifiers,\n",
    "                \"communities_with_texts\": communities_with_texts,\n",
    "                \"node_count\": graph.number_of_nodes(),\n",
    "                \"edge_count\": graph.number_of_edges()\n",
    "            }\n",
    "            print(f\"[SUCCESS] Community information collected for: {graph_name}\")\n",
    "        else:\n",
    "            print(f\"[WARN] No communities found for graph: {graph_name}\")\n",
    "\n",
    "    return all_graph_community_info\n",
    "\n",
    "\n",
    "# Collect community information for all graphs\n",
    "all_community_info = collect_graph_community_information(graphs, processed_files_directory)\n",
    "\n",
    "# Save the community information to a JSON file\n",
    "output_file = \"graph_community_information.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(all_community_info, f, indent=4)\n",
    "\n",
    "print(f\"\\n[INFO] Community information saved to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e96ee73-4b82-40e4-a52e-01b9ad45fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec\n",
    "\n",
    "def node2vec_embedding(G):\n",
    "    \"\"\"\n",
    "    Generates node embeddings for a graph using Node2Vec.\n",
    "    \"\"\"\n",
    "    n2v = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, seed=42)\n",
    "    model = n2v.fit(window=10, min_count=1, sg=1)\n",
    "    return model\n",
    "\n",
    "# Dictionary to store embeddings for all graphs\n",
    "embeddings = {}\n",
    "\n",
    "for graph_name, graph in graphs.items():\n",
    "    print(f\"\\n[INFO] Generating embeddings for graph: {graph_name}\")\n",
    "    try:\n",
    "        model = node2vec_embedding(graph)\n",
    "        embeddings[graph_name] = model\n",
    "        print(f\"[SUCCESS] Embeddings generated for: {graph_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to generate embeddings for '{graph_name}': {e}\")\n",
    "\n",
    "# Save the embeddings to files\n",
    "for graph_name, model in embeddings.items():\n",
    "    model.wv.save_word2vec_format(f\"{graph_name}_embeddings.txt\")\n",
    "    print(f\"[INFO] Embeddings saved to '{graph_name}_embeddings.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f52d5-4d1c-4b46-bca8-06c5dc225fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_community_embeddings(G, communities, model):\n",
    "    \"\"\"\n",
    "    Calculates the average embeddings for each community in the graph.\n",
    "    \"\"\"\n",
    "    community_embeddings = {}\n",
    "    for i, comm in enumerate(communities):\n",
    "        comm_nodes = list(comm)\n",
    "        comm_embeddings = [model.wv[node] for node in comm_nodes if node in model.wv]\n",
    "        if comm_embeddings:\n",
    "            avg_embedding = np.mean(comm_embeddings, axis=0)\n",
    "            community_embeddings[f\"Community {i + 1}\"] = avg_embedding\n",
    "    return community_embeddings\n",
    "\n",
    "\n",
    "community_avg_embeddings = {}\n",
    "\n",
    "for graph_name, graph in graphs.items():\n",
    "    print(f\"\\n[INFO] Calculating average embeddings for communities in: {graph_name}\")\n",
    "    \n",
    "    # Get communities and model for the graph\n",
    "    communities_with_identifiers, _ = girvan_newman_first_level_with_texts(graph, {})\n",
    "    model = embeddings.get(graph_name)\n",
    "\n",
    "    if model and communities_with_identifiers:\n",
    "        avg_embeddings = average_community_embeddings(graph, communities_with_identifiers, model)\n",
    "        community_avg_embeddings[graph_name] = avg_embeddings\n",
    "        print(f\"[SUCCESS] Average embeddings calculated for: {graph_name}\")\n",
    "    else:\n",
    "        print(f\"[ERROR] Missing model or communities for graph: {graph_name}\")\n",
    "\n",
    "# Save the average community embeddings to a file\n",
    "output_avg_embeddings_file = \"community_avg_embeddings.json\"\n",
    "with open(output_avg_embeddings_file, \"w\") as f:\n",
    "    json.dump({k: {ck: v.tolist() for ck, v in v.items()} for k, v in community_avg_embeddings.items()}, f, indent=4)\n",
    "\n",
    "print(f\"\\n[INFO] Average community embeddings saved to '{output_avg_embeddings_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66faec64-3d2a-447e-b5d7-84f10d64a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_clustering_data(community_avg_embeddings):\n",
    "    \"\"\"\n",
    "    Prepares data and labels for clustering from community average embeddings.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    for graph_name, embeddings in community_avg_embeddings.items():\n",
    "        for comm_str, embedding in embeddings.items():\n",
    "            data.append(embedding)\n",
    "            labels.append(f\"{graph_name}_{comm_str}\")\n",
    "    return np.array(data), labels\n",
    "\n",
    "\n",
    "# Prepare data for clustering\n",
    "data, labels = prepare_clustering_data(community_avg_embeddings)\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "data_tsne = tsne.fit_transform(data)\n",
    "\n",
    "# Standardise the reduced data\n",
    "scaler = StandardScaler()\n",
    "data_tsne = scaler.fit_transform(data_tsne)\n",
    "\n",
    "print(\"[INFO] Data prepared and transformed using t-SNE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98513383-6e07-4845-b092-4df5a7195127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def perform_clustering(data, bandwidth):\n",
    "    \"\"\"\n",
    "    Performs MeanShift clustering on the data with the given bandwidth.\n",
    "    \"\"\"\n",
    "    clustering = MeanShift(bandwidth=bandwidth)\n",
    "    clustering.fit(data)\n",
    "    return clustering\n",
    "\n",
    "\n",
    "def find_best_bandwidth(data, bandwidth_values):\n",
    "    \"\"\"\n",
    "    Finds the best bandwidth for MeanShift clustering using silhouette scores.\n",
    "    \"\"\"\n",
    "    best_score = -1\n",
    "    best_bandwidth = None\n",
    "    scores = []\n",
    "\n",
    "    for bandwidth in bandwidth_values:\n",
    "        try:\n",
    "            meanshift = MeanShift(bandwidth=bandwidth)\n",
    "            cluster_labels = meanshift.fit_predict(data)\n",
    "\n",
    "            num_labels = len(np.unique(cluster_labels))\n",
    "            if 2 <= num_labels < len(data):\n",
    "                score = silhouette_score(data, cluster_labels)\n",
    "                scores.append(score)\n",
    "                print(f'Bandwidth = {bandwidth}, Silhouette Score = {score:.3f}')\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_bandwidth = bandwidth\n",
    "            else:\n",
    "                scores.append(-1)\n",
    "                print(f'Bandwidth = {bandwidth}, resulted in {num_labels} clusters, not suitable for silhouette score.')\n",
    "        except Exception as e:\n",
    "            scores.append(-1)\n",
    "            print(f'[ERROR] Bandwidth = {bandwidth} failed: {e}')\n",
    "\n",
    "    return best_bandwidth, best_score, scores\n",
    "\n",
    "\n",
    "# Define bandwidth values to test\n",
    "bandwidth_values = np.linspace(0.5, 1, 10)\n",
    "\n",
    "# Find the best bandwidth and corresponding silhouette score\n",
    "best_bandwidth, best_score, scores = find_best_bandwidth(data_tsne, bandwidth_values)\n",
    "\n",
    "print(f'\\n[INFO] Best Bandwidth: {best_bandwidth}, Best Silhouette Score: {best_score:.3f}')\n",
    "\n",
    "# Plot silhouette scores vs bandwidth\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(bandwidth_values, scores, marker='o')\n",
    "plt.title('Silhouette Score vs Bandwidth for MeanShift')\n",
    "plt.xlabel('Bandwidth')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9c5145-0ae7-4ce7-94fc-38c07537818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def evaluate_silhouette_score(data, labels):\n",
    "    \"\"\"\n",
    "    Evaluates the clustering using the Silhouette Score.\n",
    "    \"\"\"\n",
    "    score = silhouette_score(data, labels)\n",
    "    print(f\"[INFO] Silhouette Score: {score:.4f}\")\n",
    "    return score\n",
    "\n",
    "\n",
    "# Perform KMeans clustering\n",
    "print(\"[INFO] Performing KMeans clustering...\")\n",
    "try:\n",
    "    kmeans = KMeans(n_clusters=15, random_state=42, n_init=10)\n",
    "    kmeans.fit(data_tsne)\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    # Evaluate clustering performance\n",
    "    silhouette_score = evaluate_silhouette_score(data_tsne, cluster_labels)\n",
    "    print(f\"[SUCCESS] KMeans clustering completed with silhouette score: {silhouette_score:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] KMeans clustering failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7294bda7-b6cf-4726-af74-5dd0a8ea2aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the clusters using t-SNE with larger figure size\n",
    "plt.figure(figsize=(100, 80))  # Increase the figure size to 100x80 inches\n",
    "unique_labels = np.unique(cluster_labels)\n",
    "colors = plt.cm.get_cmap('tab10', len(unique_labels))\n",
    "\n",
    "# Plot clusters on the t-SNE reduced data\n",
    "for label in unique_labels:\n",
    "    cluster_points = data_tsne[cluster_labels == label]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\"Cluster {label}\", c=[colors(label)], s=5000, alpha=0.8)\n",
    "\n",
    "# Title and axis labels with larger font size\n",
    "plt.title(\"t-SNE Visualization of KMeans Clustering\", fontsize=60)\n",
    "plt.xlabel(\"t-SNE Component 1\", fontsize=40)\n",
    "plt.ylabel(\"t-SNE Component 2\", fontsize=40)\n",
    "\n",
    "# Increase tick label sizes (values on the x and y axis)\n",
    "plt.tick_params(axis='x', labelsize=40)  # Increase x-axis tick label size\n",
    "plt.tick_params(axis='y', labelsize=40)  # Increase y-axis tick label size\n",
    "\n",
    "# Adjust legend size and location\n",
    "plt.legend(fontsize=40, loc='best')\n",
    "\n",
    "# Group labels into clusters\n",
    "clusters = {i: [] for i in range(len(unique_labels))}\n",
    "\n",
    "for idx, cluster_label in enumerate(cluster_labels):\n",
    "    clusters[cluster_label].append(labels[idx])\n",
    "\n",
    "# Print community details with actual text\n",
    "print(\"\\n[INFO] Cluster details with community texts:\")\n",
    "\n",
    "for cluster_label, communities in clusters.items():\n",
    "    print(f\"\\nCluster {cluster_label}:\")\n",
    "    for community in communities:\n",
    "        # Extract the graph name and community number from the label\n",
    "        parts = community.split('_', 1)  # Split into graph_name and the rest (community ID)\n",
    "        \n",
    "        if len(parts) == 2:\n",
    "            graph_name, comm_str = parts\n",
    "        else:\n",
    "            # Handle cases where the label does not follow the expected format\n",
    "            print(f\"Skipping invalid community label: {community}\")\n",
    "            continue\n",
    "        \n",
    "        # Get the corresponding community number from the label\n",
    "        community_number = comm_str.split()[-1]  # Extract the number, e.g., '8' from 'Community 8'\n",
    "        \n",
    "        # Get the corresponding text content for the community based on the community number\n",
    "        community_text = rectangle_text_mapping.get(community_number, f\"[Text not found for {community_number}]\")\n",
    "        \n",
    "        print(f\"  - {community}: {community_text}\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b14a20-5164-4129-b3b3-1c81e6c22147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (accimap_env)",
   "language": "python",
   "name": "accimap_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
